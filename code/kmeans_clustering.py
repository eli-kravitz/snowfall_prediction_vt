'''Do k-means clustering on data and save data off in good format for hierarchicalclustering in R.'''import osimport pandas as pdimport picklefrom matplotlib import pyplot as pltfrom sklearn.preprocessing import StandardScalerfrom sklearn.decomposition import PCAfrom sklearn.cluster import KMeansimport numpy as npfrom sklearn.metrics import silhouette_scoreimport pickledef load_data(path):        '''    Description:        Load saved pickle files        Inputs:        path: str            full path to file        Outputs:        data: pd.DataFrame            location data    '''            # Open with pickle    with open(path, 'rb') as input_file:        data = pickle.load(input_file)            return datadef split_data_by_year(data, start_year, end_year):        '''    Description:        Split dataframe into years        Inputs:        data: pd.DataFrame            all location data        start_year: int            year data starts        end_year: int            year data ends        Outputs:        data_yearly: list(dtype=pd.DataFrame)            list of pandas dataframes separated by year    '''        data_yearly = [[]] * len(range(start_year, end_year))    for (i, year) in enumerate(range(start_year, end_year)):                # Get data from given year        tmp = data[data['time'].str.contains(str(year))]        tmp = tmp.sort_values(by=['days_since_Jan1_2000'])        tmp = tmp.reset_index(drop=True)        data_yearly[i] = tmp                # Add season label        season = []        for date in data_yearly[i]['time']:            split_date = date.split('-')            month = int(split_date[1])            day = int(split_date[2].split('T')[0])            if (month == 3 and day >= 20) or (month == 4) or (month == 5) or (month == 6 and day < 20):                season.append('Spring')            elif (month == 6 and day >= 20) or (month == 7) or (month == 8) or (month == 9 and day < 20):                season.append('Summer')            elif (month == 9 and day >= 20) or (month == 10) or (month == 11) or (month == 12 and day < 20):                season.append('Fall')            else:                season.append('Winter')                        data_yearly[i]['season'] = season            return data_yearlydef standardize_data(data_yearly):        '''    Description:        Standardize yearly data        Inputs:        data_yearly: list(dtype=pd.DataFrame)            list of pandas dataframes separated by year        Outputs:        data_std: list(dtype=np.array(dtype=float))            '''        scale = StandardScaler()    data_std = [[]] * len(data_yearly)    for i in range(len(data_yearly)):        data_std[i] = scale.fit_transform(data_yearly[i][[            'direct_radiation',            'soil_temperature_0_to_7cm',            'temperature_2m']])            return data_std# Get all cleaned data files pwd = os.getcwd()path = os.path.join(pwd, 'api_data_clean')files = os.listdir(path)# Get datafor (i, f) in enumerate(files):        file = os.path.join(path, f)    if i == 0:        data = load_data(file)    else:        data_tmp = load_data(file)        data = pd.concat([data, data_tmp], axis=0)        # Split into yearsdata_yearly = split_data_by_year(data, 2013, 2023)# Normalize data so it's on the same scaledata_std = standardize_data(data_yearly)# Save datapwd = os.getcwd()save_loc = os.path.join(pwd, 'clustering_data')if not os.path.isdir(save_loc):    os.mkdir(save_loc)file_name = 'standardized_data.pkl'full_file = os.path.join(save_loc, file_name)with open(full_file, 'wb') as handle:    pickle.dump(data_std, handle, protocol=pickle.HIGHEST_PROTOCOL)file_name = 'yearly_data.pkl'full_file = os.path.join(save_loc, file_name)with open(full_file, 'wb') as handle:    pickle.dump(data_yearly, handle, protocol=pickle.HIGHEST_PROTOCOL)    # Save data_yearly in csv files - only 50 points from each season for hierarchicalsave_loc = os.path.join(pwd, 'clustering_data', 'csv')if not os.path.isdir(save_loc):    os.mkdir(save_loc)year = 2013for d in data_yearly:    save_df = pd.DataFrame(columns=d.columns)    full_file = os.path.join(save_loc, str(year) + '.csv')    year += 1    for s in ['Summer', 'Fall', 'Winter', 'Spring']:                # Get data from current season        tmp_df = d.loc[d['season'] == s]                # Get 50 random rows from dataframe        tmp_df = tmp_df.sample(n = 50)                # Concatenate to save        save_df = pd.concat([save_df, tmp_df])               save_df.to_csv(full_file)# Get some folder infopwd = os.getcwd()save_loc = os.path.join(pwd, 'figs', 'clustering')if not os.path.isdir(save_loc):    os.mkdir(save_loc)    # Do PCA and plot to see number of dimensions neededexplained_variance_yearly = [[]] * len(data_std)silhouette_avg_yearly = [[]] * len(data_std)years = np.arange(2013, 2023)for (idx, d) in enumerate(data_std):    pca = PCA()    pca.fit(d)    explained_variance_yearly[idx] = pca.explained_variance_ratio_        # Get number of components with >80% variance explained    n_comp = pca.explained_variance_ratio_.cumsum()    n_comp = n_comp > 0.8    n_comp = [i for (i, x) in enumerate(n_comp) if x]    n_comp = n_comp[0] + 1        # Now do kmeans with n_comp    pca = PCA(n_components = n_comp)    pca.fit(d)    pca_data = pca.transform(d)        # Define max number of mixands, use static number    k_max = 6    k_vec = list(range(2, k_max))        silhouette_avg = np.array([])    for k in k_vec:                # Run Lloyd's algorithm initializing with K-means++ to choose         # centers and running 5 times for each centroid seed        kmeans = KMeans(n_clusters=k, init='k-means++', n_init=5).fit(pca_data)                # Find silhouette score with Euclidean distance        #score = silhouette_score(pca_data, kmeans.labels_)        score = silhouette_score(d, kmeans.labels_)        silhouette_avg = np.append(silhouette_avg, score)        silhouette_avg_yearly[idx] = silhouette_avg            # Get maximum silhouette score and best K    max_idx = np.argmax(silhouette_avg)    k_final = k_vec[max_idx]        # Run Lloyd's algorithm initializing with K-means++ to choose     # centers and running 10 times for each centroid seed    kmeans = KMeans(n_clusters=k_final, init='k-means++', n_init=10).fit(pca_data)        # Save true and kmeans labels    true_labels = np.array(data_yearly[idx]['season'])    kmeans_labels = list(kmeans.labels_)    fig, ax = plt.subplots()    if idx == 0:        for_bar = np.zeros(shape=(len(np.unique(true_labels)), len(np.unique(kmeans_labels))))        for (idx1, lab) in enumerate(np.unique(kmeans.labels_)):            msk = kmeans_labels == lab            vals = list(true_labels[msk])            for (idx2, truth) in enumerate(np.unique(true_labels)):                indices = [i for (i, x) in enumerate(vals) if x == truth]                val = len(indices)                for_bar[idx2][idx1] = val                    # Plot bar graph        plt.figure()        w = 0.1        x = np.unique(kmeans_labels)        plt.bar(x - w - w /2, for_bar[0], width=w, label=np.unique(true_labels)[0])        plt.bar(x - w / 2, for_bar[1], width=w, label=np.unique(true_labels)[1])        plt.bar(x + w / 2, for_bar[2], width=w, label=np.unique(true_labels)[2])        plt.bar(x + w + w / 2, for_bar[3], width=w, label=np.unique(true_labels)[3])        plt.grid()        plt.legend()        plt.xlabel('Cluster Label')        plt.ylabel('Counts')        plt.title('True Seasons by Cluster')        plt.xticks(np.unique(kmeans_labels))        plt.savefig(os.path.join(save_loc, '2013_cluster_true.png'), dpi=300)                # Plot results with first 2 principal components    x = pca_data[:, 0]    y = pca_data[:, 1]    labels = kmeans.labels_    fig, ax = plt.subplots()    scatter = ax.scatter(x, y, c=labels)    plt.xlabel('Principal Component 1')    plt.ylabel('Principal Component 2')    plt.title('Kmeans Clustering: ' + str(years[idx]))    plt.grid()    leg = ax.legend(*scatter.legend_elements(), title='Clusters')    ax.add_artist(leg)    plt.savefig(os.path.join(save_loc, 'kmeans_' + str(years[idx]) + '.png'), dpi=300)            # Plot explained variance by yearcolors = ['tab:blue',          'tab:orange',          'tab:green',          'tab:red',          'tab:purple',          'tab:brown',          'tab:pink',          'tab:gray',          'tab:olive',          'tab:cyan']plt.figure(figsize=(8, 6))for (idx, evy) in enumerate(explained_variance_yearly):    plt.plot(range(1, len(evy) + 1),             evy.cumsum(), c=colors[idx], label=years[idx])plt.xlabel('# Components')plt.ylabel('Cumulative Explained Variance')plt.title('Explained Variance by # Components')plt.grid()plt.legend()ax = plt.gca()ax.set_xticks(range(1, len(evy) + 1))plt.savefig(os.path.join(save_loc, 'pca_variance.png'), dpi=300)# Plot Silhouette score by yearplt.figure(figsize=(8, 6))for (idx, say) in enumerate(silhouette_avg_yearly):    plt.plot(k_vec, say, c=colors[idx], label=years[idx])plt.legend(loc='upper right')plt.title('Silhouette Score')plt.xlabel('# Clusters')plt.ylabel('Silhouette Score')plt.grid()plt.savefig(os.path.join(save_loc, 'silhouette.png'), dpi=300)                                            