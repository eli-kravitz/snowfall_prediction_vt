'''Use NBC to predict snowfall given (independent) input features'''import numpy as npimport sklearn as skimport osimport pickleimport pandas as pdimport matplotlib.pyplot as pltimport randomimport seaborn as snsfrom sklearn.metrics import confusion_matrixclass NaiveBayes():        def __init__(self, x, y, nb_type):                '''        Description:            Initialize NBC class                Inputs:            x: np.array(dtype=np.array(dtype=float))                feature vectors            y: np.array(dtype=str)                labels            nb_type: str                string of type                    1. gaussian                    2. multinomial                Outputs:            N/A        '''                # Get training indices: 70% of total        total_vals = len(y)        percent_70 = round(0.7 * total_vals)        train_idx = random.sample(range(total_vals), percent_70)                self.train_x = x[train_idx]        self.train_y = y[train_idx]        self.test_x = np.delete(x, train_idx, axis=0)        self.test_y = np.delete(y, train_idx, axis=0)        self.nb_type = nb_type                if nb_type == 'gaussian':            self.model = sk.naive_bayes.GaussianNB()        elif nb_type == 'multinomial':            self.model = sk.naive_bayes.MultinomialNB()        else:            print('Unrecognized string, using Gaussian NBC')            self.model = sk.naive_bayes.GaussianNB()            def train(self):                '''        Description:            Train multinomial NBC                Inputs:            N/A                Outputs:            N/A        '''                self.model.fit(self.train_x, self.train_y)def clean_nbc_data(data):        '''    Description:        Put yearly data into one dataframe and clean up columns        Inputs:        data: list(dtype=pd.DataFrame)            list of pandas dataframes separated by year        Outputs:        data_clean: pd.DataFrame            all location data    '''        # Initialize dataframe    data_clean = pd.DataFrame(columns=data[0].columns)        # Add to one big dataframe    for d in data:        data_clean = pd.concat([data_clean, d])            # Renumber indices    data_clean = data_clean.reset_index(drop=True)        # Get rid of columns we don't want    data_clean = data_clean.drop(['time', 'days_since_Jan1_2000',                                   'season'], axis=1)        return data_cleandef make_categorical_labels(data):        '''    Description:        Make categorical snowfall labels        Inputs:        data: pd.DataFrame            all location data        Outputs:        data: pd.DataFrame            all location data with categorical snowfall labels    '''        # Define categorical labels for hourly snowfall    labels = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])        snowfall_data = data['snowfall']        # Make column string    data['snowfall'] = data['snowfall'].astype(str)        # Change to categorical values    for (idx, val) in enumerate(snowfall_data):                # Get new label        if val == 0:            lab = '0'        elif val >= 1:            lab = '>1'        else:            # Find nearest discrete value, rounding down            tmp = labels - val            if sum(tmp == 0) > 0:                msk = tmp == 0                i = np.argmax(msk)            else:                msk = tmp >= 0                i = np.argmax(msk) - 1            lab = str(labels[i]) + '-' + str(labels[i + 1])                        # Save label to dataframe        data.loc[idx, 'snowfall'] = lab                 return dataif __name__ == "__main__":        nbc_type = 'multinomial'    # Make location to save data    pwd = os.getcwd()    save_loc = os.path.join(pwd, 'nbc_data')    if not os.path.isdir(save_loc):        os.mkdir(save_loc)            file_name = 'nbc_cat_data.pkl'    full_file = os.path.join(save_loc, file_name)        if not os.path.isfile(full_file):                # Load in data        data_loc = os.path.join(pwd, 'clustering_data', 'yearly_data.pkl')        with open(data_loc, 'rb') as input_file:            data = pickle.load(input_file)                    # Clean up data        data_clean = clean_nbc_data(data)                # Make snowfall have categorical labels        data_cat = make_categorical_labels(data_clean)                # Save data        with open(full_file, 'wb') as handle:            pickle.dump(data_cat, handle, protocol=pickle.HIGHEST_PROTOCOL)                else:                # Load data        with open(full_file, 'rb') as input_file:            data_cat = pickle.load(input_file)        # Get labels    y = np.array(data_cat['snowfall'])        # Pop out snowfall column    data_cat = data_cat.drop(['snowfall'], axis=1)        if nbc_type == 'gaussian':                # Get features        x = data_cat.to_numpy()                # Train NBC        obj = NaiveBayes(x, y, nbc_type)        obj.train()        else:                # Make new dataframe for multinomial NBC        cols = data_cat.columns        new_cols = []        for c in cols:            new_cols.append(c + '_l')            new_cols.append(c + '_m')            new_cols.append(c + '_h')        df = pd.DataFrame(0, index=np.arange(len(data_cat)), columns=new_cols)        for c in cols:            data = data_cat[c]            q1 = np.quantile(data, 0.25)            q3 = np.quantile(data, 0.75)            df.loc[data <= q1, c + '_l'] = 1            df.loc[data >= q3, c + '_h'] = 1            df.loc[np.logical_and(data > q1, data < q3), c + '_m'] = 1                # Save data        file_name = 'nbc_all_cat_data.pkl'        full_file = os.path.join(save_loc, file_name)        with open(full_file, 'wb') as handle:            pickle.dump(data_cat, handle, protocol=pickle.HIGHEST_PROTOCOL)                x = df.to_numpy()                obj = NaiveBayes(x, y, nbc_type)        obj.train()        # Make confusion matrix    names = np.unique(obj.train_y)    pred = obj.model.predict(obj.test_x)    truth = obj.test_y    confuse = confusion_matrix(truth, pred)    confuse = confuse.astype('float')    for (i, r) in enumerate(confuse):        confuse[i] = r / sum(r)    plt.figure(figsize=(20, 20))    fx = sns.heatmap(confuse, annot=True, fmt='.2f', cmap='GnBu')    fx.set_title('Confusion Matrix \n');    fx.set_xlabel('\n Predicted Values\n')    fx.set_ylabel('Actual Values\n');    fx.xaxis.set_ticklabels(names)    fx.yaxis.set_ticklabels(names)        pwd = os.getcwd()        save_loc = os.path.join(pwd, 'figs', 'nbc')    if not os.path.isdir(save_loc):        os.mkdir(save_loc)            plt.savefig(os.path.join(save_loc, 'confusion.png'), dpi=300)