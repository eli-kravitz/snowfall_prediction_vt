'''Predict Burlington snowfall for 5 years'''import pickleimport osimport matplotlib.pyplot as pltimport numpy as npfrom classifier import Classifier # need this to load NN modelimport globimport torchimport randomimport pandas as pdimport datetime as dtif __name__ == '__main__':        '''    Start by getting NN model and sampled time series data.    '''        # Load NN model    pwd = os.getcwd()    mod_loc = os.path.join(pwd, 'NN_data', 'deep_NN_model.pkl')    with open(mod_loc, 'rb') as input_file:        nn_model = pickle.load(input_file)        # Make location to save sim data    save_sim = os.path.join(pwd, 'sim_data')    if not os.path.isdir(save_sim):        os.mkdir(save_sim)    sim_file = os.path.join(save_sim, 'sim_data_burlington.pkl')        if not os.path.isfile(sim_file):                # Load time series forecasting models        # Make sure these are in the order NN model expects!        cols = ['temperature_2m', 'relativehumidity_2m', 'apparent_temperature',               'surface_pressure', 'shortwave_radiation', 'direct_radiation',               'diffuse_radiation', 'direct_normal_irradiance', 'windspeed_10m',               'winddirection_10m', 'et0_fao_evapotranspiration',               'soil_temperature_0_to_7cm', 'soil_moisture_0_to_7cm']        tsf_models = [[]] * len(cols)        tsf_loc = os.path.join(pwd, 'TSF_data')        files = glob.glob(os.path.join(tsf_loc, '*'))        for f in files:            name = f.split('/')[-1]            name = name.split('.')[0]            for idx, col in enumerate(cols):                if name == col:                    break            with open(f, 'rb') as input_file:                tsf_models[idx] = pickle.load(input_file)                # Loop through models and get 3 years worth of predictions        pred_data = [[]] * len(cols)        hours = 8760 * 3        for (i, model) in enumerate(tsf_models):                        future = model.make_future_dataframe(periods=hours, freq='h')                        # Only want predictions on new points            future = future[-hours:]                        # Predictions with uncertainty            forecast = model.predict(future)            samples = model.predictive_samples(future)                    # Define physical bounds on data            c = cols[i]            if c == 'temperature_2m':                cap_val = 50.                floor_val = -50.            elif c == 'relativehumidity_2m':                cap_val = 100.                floor_val = 0.            elif c == 'apparent_temperature':                cap_val = 50.                floor_val = -50.            elif c == 'surface_pressure':                cap_val = 1035.                floor_val = 950.            elif c == 'shortwave_radiation':                cap_val = 1000.                floor_val = 0.            elif c == 'direct_radiation':                cap_val = 900.                floor_val = 0.            elif c == 'diffuse_radiation':                cap_val = 500.                floor_val = 0.            elif c == 'direct_normal_irradiance':                cap_val = 1020.                floor_val = 0.            elif c == 'windspeed_10m':                cap_val = 60.                floor_val = 0.            elif c == 'winddirection_10m':                cap_val = 360.                floor_val = 0.            elif c == 'et0_fao_evapotranspiration':                cap_val = 1.                floor_val = 0.            elif c == 'soil_temperature_0_to_7cm':                cap_val = 35.                floor_val = -15.            elif c == 'soil_moisture_0_to_7cm':                cap_val = 0.4                floor_val = 0.1            else:                print('Unrecognized string.')                            # Impose physical bounds on data            sample_data = samples['yhat']            sample_data[sample_data > cap_val] = cap_val            sample_data[sample_data < floor_val] = floor_val                        # Store samples            pred_data[i] = sample_data                    # Save samples        with open(sim_file, 'wb') as handle:            pickle.dump(pred_data, handle, protocol=pickle.HIGHEST_PROTOCOL)                else:                with open(sim_file, 'rb') as input_file:            pred_data = pickle.load(input_file)               '''    Now:        1. Sample 50 points from each feature at each time        2. Run through NN classifier        3. Choose snowfall amount based on max likelihood from NN output        4. Store 50 snowfall samples at each time        5. Plot with confidence intervals    '''        save_snow = os.path.join(save_sim, 'snow_data_burlington.pkl')        if not os.path.isfile(save_snow):                # Normalize data        for i in range(len(pred_data)):            pred_data[i] = (pred_data[i] - pred_data[i].mean()) / pred_data[i].std()                # Make dict of index to snowfall        snow_dict = {0:  0.00,                      1:  0.05,                      2:  0.15,                     3:  0.25,                      4:  0.35,                      5:  0.45,                     6:  0.55,                      7:  0.65,                      8:  0.75,                     9:  0.85,                      10: 0.95,                      11: 1.05}                n_sample = 50                # Store all snow data        snow_data = np.ndarray(shape=(len(pred_data[0]), n_sample))                # Loop through each time        for i in range(len(pred_data[0])):                        # Sample n_sample points from each feature            for j in range(n_sample):                                nn_data = np.zeros(len(pred_data))                                for k in range(len(pred_data)):                                        nn_data[k] = np.random.choice(pred_data[k][i], size=1)                                    # Now classify this point                nn_data = torch.tensor(nn_data, dtype=torch.float)                nn_out = torch.exp(nn_model(nn_data))                nn_out = nn_out.detach().numpy()                                # Sample from this output                '''                best_sample = random.choices(population=range(len(nn_out)),                                              weights=nn_out)[0]                '''                best_sample = np.argmax(nn_out)                                snow_sample = snow_dict[best_sample]                snow_data[i][j] = snow_sample                        # Save samples        with open(save_snow, 'wb') as handle:            pickle.dump(snow_data, handle, protocol=pickle.HIGHEST_PROTOCOL)                else:                with open(save_snow, 'rb') as input_file:            snow_data = pickle.load(input_file)                # Now plot    pwd = os.getcwd()    save_loc = os.path.join(pwd, 'api_data_clean')    file_name = 'Burlington.pkl'    full_file = os.path.join(save_loc, file_name)    with open(full_file, 'rb') as input_file:        data_full = pickle.load(input_file)    time = pd.to_datetime(data_full['time'])    t_start = time.iloc[-1]    hours = 8760 * 3    t_vec = pd.date_range(t_start, periods=hours, freq='h')    lower = np.quantile(snow_data, 0.025, axis=1)    upper = np.quantile(snow_data, 0.975, axis=1)    center = np.mean(snow_data, axis=1)    plt.figure()    plt.plot(time, data_full['snowfall'], label='Historical Data')    plt.plot(t_vec, center, label='Mean Predicted Data')    plt.fill_between(t_vec, lower, upper,                      alpha=0.5, label='95% Confidence Interval')    plt.grid()    plt.xlabel('Date')    plt.ylabel('Snowfall')    plt.title('Snowfall by Date')    save_fig = os.path.join(pwd, 'figs', 'simulation')    if not os.path.isdir(save_fig):        os.mkdir(save_fig)    file_name = 'pred_daily.png'    full_file = os.path.join(save_fig, file_name)    plt.savefig(full_file, dpi=300, bbox_inches='tight')        # Plot yearly snowfall for historcial data and predictions with confidence    # bounds on it    yearly_data_hist = [[]] * 10    string_date = data_full['time']    year_list = []    for i in range(len(string_date)):        year_list.append(dt.datetime.strptime(data_full['time'][i],                                             '%Y-%m-%dT%H:%M').year)    for (i, year) in enumerate(range(2013, 2023)):        msk = np.array(year_list) == year        year_data = data_full[msk]        yearly_data_hist[i] = sum(year_data['snowfall'])            yearly_data_new = np.ndarray(shape=(3, 3))    for (i, year) in enumerate(range(2023, 2026)):        msk = t_vec.year==year        year_data = snow_data[msk]        yearly_data_new[i][0] = sum(np.mean(year_data, axis=1))        yearly_data_new[i][1] = sum(np.quantile(year_data, 0.025, axis=1))        yearly_data_new[i][2] = sum(np.quantile(year_data, 0.975, axis=1))        mean = yearly_data_new[:, 0]    err = yearly_data_new[:, 1:3].T    err[0, :] = err[0, :] + mean    err[1, :] = err[1, :] - mean    plt.figure()    plt.plot(list(range(2013, 2023)), yearly_data_hist, label='Historical',             marker='.', ms=8)    plt.errorbar(list(range(2023, 2026)), mean, err, marker='s',                  ms=5, capsize=4, label='Predicted')    plt.grid()    plt.legend(loc='upper left')    plt.xlabel('Year')    plt.ylabel('Snowfall [in]')    file_name = 'pred_yearly.png'    full_file = os.path.join(save_fig, file_name)    plt.savefig(full_file, dpi=300, bbox_inches='tight')                        