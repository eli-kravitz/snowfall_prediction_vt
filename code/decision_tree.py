'''Use NBC to predict snowfall given (independent) input features'''import numpy as npimport sklearn.tree as treeimport osimport pickleimport pandas as pdimport matplotlib.pyplot as pltimport randomimport seaborn as snsfrom sklearn.metrics import confusion_matrixclass DecisionTree():        def __init__(self, x, y, depth):                '''        Description:            Initialize decision tree class                Inputs:            x: np.array(dtype=np.array(dtype=float))                feature vectors            y: np.array(dtype=str)                labels            depth: int                maximum depth of tree                Outputs:            N/A        '''                # Get training indices: 70% of total        total_vals = len(y)        percent_70 = round(0.7 * total_vals)        train_idx = random.sample(range(total_vals), percent_70)                self.train_x = x[train_idx]        self.train_y = y[train_idx]        self.test_x = np.delete(x, train_idx, axis=0)        self.test_y = np.delete(y, train_idx, axis=0)        self.model = tree.DecisionTreeClassifier(max_depth=depth)            def train(self):                '''        Description:            Train multinomial NBC                Inputs:            N/A                Outputs:            N/A        '''                self.model.fit(self.train_x, self.train_y)def clean_data(data):        '''    Description:        Put yearly data into one dataframe and clean up columns        Inputs:        data: list(dtype=pd.DataFrame)            list of pandas dataframes separated by year        Outputs:        data_clean: pd.DataFrame            all location data    '''        # Initialize dataframe    data_clean = pd.DataFrame(columns=data[0].columns)        # Add to one big dataframe    for d in data:        data_clean = pd.concat([data_clean, d])            # Renumber indices    data_clean = data_clean.reset_index(drop=True)        # Get rid of columns we don't want    data_clean = data_clean.drop(['time', 'days_since_Jan1_2000',                                   'season'], axis=1)        return data_cleandef make_categorical_labels(data):        '''    Description:        Make categorical snowfall labels        Inputs:        data: pd.DataFrame            all location data        Outputs:        data: pd.DataFrame            all location data with categorical snowfall labels    '''        # Define categorical labels for hourly snowfall    labels = np.array([0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])        snowfall_data = data['snowfall']        # Make column string    data['snowfall'] = data['snowfall'].astype(str)        # Change to categorical values    for (idx, val) in enumerate(snowfall_data):                # Get new label        if val == 0:            lab = '0'        elif val >= 1:            lab = '>1'        else:            # Find nearest discrete value, rounding down            tmp = labels - val            if sum(tmp == 0) > 0:                msk = tmp == 0                i = np.argmax(msk)            else:                msk = tmp >= 0                i = np.argmax(msk) - 1            lab = str(labels[i]) + '-' + str(labels[i + 1])                        # Save label to dataframe        data.loc[idx, 'snowfall'] = lab                 return dataif __name__ == "__main__":    # Make location to save data    pwd = os.getcwd()    save_loc = os.path.join(pwd, 'nbc_data')    if not os.path.isdir(save_loc):        os.mkdir(save_loc)            file_name = 'nbc_cat_data.pkl'    full_file = os.path.join(save_loc, file_name)        if not os.path.isfile(full_file):                # Load in data        data_loc = os.path.join(pwd, 'clustering_data', 'yearly_data.pkl')        with open(data_loc, 'rb') as input_file:            data = pickle.load(input_file)                    # Clean up data        data_clean = clean_data(data)                # Make snowfall have categorical labels        data_cat = make_categorical_labels(data_clean)                # Save data        with open(full_file, 'wb') as handle:            pickle.dump(data_cat, handle, protocol=pickle.HIGHEST_PROTOCOL)                else:                # Load data        with open(full_file, 'rb') as input_file:            data_cat = pickle.load(input_file)        # Get labels    y = np.array(data_cat['snowfall'])        # Pop out snowfall column    data_cat = data_cat.drop(['snowfall'], axis=1)            # Get features    x = data_cat.to_numpy()            # Make and save plots    save_loc = os.path.join(pwd, 'figs', 'decision_tree')    if not os.path.isdir(save_loc):        os.mkdir(save_loc)        # Train decision tree 3 times    for k in range(0, 3):                obj = DecisionTree(x, y, 3)        obj.train()                plt.figure(figsize=(50, 30))        tree.plot_tree(obj.model, fontsize=10)        plt.savefig(os.path.join(save_loc, 'tree' + str(k) + '.png'), dpi=300)                # Make confusion matrix        names = np.unique(obj.train_y)        pred = obj.model.predict(obj.test_x)        truth = obj.test_y        confuse = confusion_matrix(truth, pred)        confuse = confuse.astype('float')        for (i, r) in enumerate(confuse):            confuse[i] = r / sum(r)        plt.figure(figsize=(20, 20))        fx = sns.heatmap(confuse, annot=True, fmt='.2f', cmap='GnBu')        fx.set_title('Confusion Matrix \n');        fx.set_xlabel('\n Predicted Values\n')        fx.set_ylabel('Actual Values\n');        fx.xaxis.set_ticklabels(names)        fx.yaxis.set_ticklabels(names)        plt.savefig(os.path.join(save_loc, 'confusion' + str(k) + '.png'), dpi=300)            # Confusion matrix for deeper tree    obj = DecisionTree(x, y, 500)    obj.train()        # Make confusion matrix    names = np.unique(obj.train_y)    pred = obj.model.predict(obj.test_x)    truth = obj.test_y    confuse = confusion_matrix(truth, pred)    confuse = confuse.astype('float')    for (i, r) in enumerate(confuse):        confuse[i] = r / sum(r)    plt.figure(figsize=(20, 20))    fx = sns.heatmap(confuse, annot=True, fmt='.2f', cmap='GnBu')    fx.set_title('Confusion Matrix \n');    fx.set_xlabel('\n Predicted Values\n')    fx.set_ylabel('Actual Values\n');    fx.xaxis.set_ticklabels(names)    fx.yaxis.set_ticklabels(names)    plt.savefig(os.path.join(save_loc, 'confusion_full.png'), dpi=300)        