'''Make SVM classifier'''import numpy as npimport osimport pickleimport matplotlib.pyplot as pltfrom sklearn.metrics import confusion_matrixfrom sklearn.decomposition import PCAfrom sklearn.model_selection import train_test_splitfrom sklearn.svm import SVCfrom sklearn.model_selection import GridSearchCVfrom sklearn.preprocessing import StandardScalerfrom mlxtend.plotting import plot_decision_regions# Location of data filepwd = os.getcwd()save_loc = os.path.join(pwd, 'nbc_data')    file_name = 'nbc_cat_data.pkl'full_file = os.path.join(save_loc, file_name)with open(full_file, 'rb') as input_file:    data_cat = pickle.load(input_file)    # Get labelsy_str = np.array(data_cat['snowfall'])# Convert labels to binaryy = np.ones(len(y_str), dtype=int)msk = y_str == '0'y[msk] = 0# Pop out snowfall columndata_cat = data_cat.drop(['snowfall'], axis=1)'''1. Linear kernel with varying cost    - Confusion matrix with best cost    - Accuracy with best cost2. Polynomial kernel with varying cost    - Confusion matrix with best cost    - Accuracy with best cost3. RBF kernel with varying cost    - Confusion matrix with best cost    - Accuracy with best cost4. CHoose most accurate classifier, do PCA, and show decision boundary'''# Need to limit the size # Do PCA in order to make dataset visualizable and make things faster - # keep 80% explained variancepca = PCA()pca.fit(data_cat)# Get number of components with 80% variance explainedn_comp = pca.explained_variance_ratio_.cumsum()n_comp = n_comp > 0.9n_comp = [i for (i, x) in enumerate(n_comp) if x]n_comp = n_comp[0] + 1# Get principal componentspca = PCA(n_components = n_comp)pca.fit(data_cat)x = pca.transform(data_cat)# Build SVM with principal componentsx_train, x_test, y_train, y_test = train_test_split(x, y,    test_size=0.99, random_state=42)# Scale datascaler = StandardScaler()x_train = scaler.fit_transform(x_train)x_test = scaler.fit_transform(x_test)# Plot PCs colored by labelfig, ax = plt.subplots()ax.scatter(x_train[:, 0], x_train[:, 1], c=y_train, alpha=0.3)ax.set_xlabel('Principal Component 1')ax.set_ylabel('Principal Component 1')plt.grid()plt.title('Training Data Colored by Label')# Linear kernelparam_grid = {'C': [0.1, 1, 10, 100, 1000],              'kernel': ['linear']} grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)grid.fit(x_train, y_train)best_params_lin = grid.best_params_# Polynomial kernelparam_grid = {'C': [0.1, 1, 10, 100, 1000],                'gamma': [1, 0.1, 0.01, 0.001, 0.0001],               'degree': [2, 3, 4],              'kernel': ['poly']} grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)grid.fit(x_train, y_train)best_params_poly = grid.best_params_# RBF kernelparam_grid = {'C': [0.1, 1, 10, 100, 1000],                'gamma': [1, 0.1, 0.01, 0.001, 0.0001],               'kernel': ['rbf']} grid = GridSearchCV(SVC(), param_grid, refit=True, verbose=3)grid.fit(x_train, y_train)best_params_rbf = grid.best_params_'''x1 = np.random.multivariate_normal(np.array([0, 0]), np.array([[1, 0], [0, 1]]), 100)y1 = np.zeros(len(x1), dtype=int)x2 = np.random.multivariate_normal(np.array([2, 2]), np.array([[1, 0], [0, 1]]), 100)y2 = np.ones(len(x2), dtype=int)x_train = np.vstack((x1, x2))y_train = np.concatenate((y1, y2))''''''model_lin = SVC(kernel='linear', random_state=1, C=0.1)clf_lin = model_lin.fit(x_train, y_train)pred = model_lin.predict(x_test)c_lin = confusion_matrix(y_test, pred)plot_decision_regions((x_train), (y_train), clf=clf_lin, legend=2)model_poly = SVC(kernel='poly', degree=3, gamma='auto', coef0=1, C=0.1)clf_poly = model_poly.fit(x_train, y_train)pred = model_poly.predict(x_test)c_poly = confusion_matrix(y_test, pred)plot_decision_regions((x_train), (y_train), clf=clf_lin, legend=2)model_rbf = SVC(kernel='rbf', gamma=0.5, C=0.1)model_rbf.fit(x_train, y_train)pred = model_rbf.predict(x_test)c_rbf = confusion_matrix(y_test, pred)plot_decision_regions((x_train), (y_train), clf=clf_lin, legend=2)'''